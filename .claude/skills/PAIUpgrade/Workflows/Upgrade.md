# Upgrade Workflow

## Voice Notification

```bash
curl -s -X POST http://localhost:8888/notify \
  -H "Content-Type: application/json" \
  -d '{"message": "Running the Upgrade workflow in the PAIUpgrade skill to check for upgrades"}' \
  > /dev/null 2>&1 &
```

Running the **Upgrade** workflow in the **PAIUpgrade** skill to check for upgrades...

**Primary workflow for PAIUpgrade skill.** Generates prioritized upgrade recommendations by running two parallel agent threads: user context analysis and source collection.

**Trigger:** "check for upgrades", "upgrade", "any updates", "check Anthropic", "check YouTube", "pai upgrade"

---

## Overview

This workflow executes the core PAIUpgrade pattern:

1. **Thread 1:** Analyze user context (TELOS, projects, recent work, PAI state)
2. **Thread 2:** Collect updates from sources (Anthropic, YouTube, custom)
3. **Synthesize:** Combine context + discoveries into personalized recommendations
4. **Output:** Prioritized upgrade report

Both threads run in parallel for efficiency.

---

## Execution

### Step 1: Launch Thread 1 - User Context Analysis

Spawn 4 parallel Intern agents to gather user context:

```
Use Task tool with subagent_type=Intern, run 4 agents in parallel:

Agent 1 - TELOS Analysis:
"Read and analyze the user's TELOS files to understand their current focus:
- ~/.claude/skills/PAI/USER/TELOS/TELOS.md
- ~/.claude/skills/PAI/USER/TELOS/GOALS.md
- ~/.claude/skills/PAI/USER/TELOS/PROJECTS.md
- ~/.claude/skills/PAI/USER/TELOS/CHALLENGES.md
- ~/.claude/skills/PAI/USER/TELOS/STATUS.md

Extract and return:
1. Current high-priority goals
2. Active focus areas
3. Key challenges they're working on
4. Project themes and directions

Format as structured JSON."

Agent 2 - Recent Work Analysis:
"Analyze the user's recent work patterns:
- Read ~/.claude/MEMORY/STATE/current-work.json
- Check recent MEMORY/WORK/ directories (last 7 days)

Extract and return:
1. What projects they've been actively working on
2. Patterns in their work (what keeps coming up)
3. Any open/incomplete tasks
4. Recent accomplishments

Format as structured JSON."

Agent 3 - PAI System State:
"Analyze the current state of the user's PAI system:
- List skills in ~/.claude/skills/
- List hooks in ~/.claude/hooks/
- Read ~/.claude/settings.json

Extract and return:
1. Installed skills (list with brief purpose)
2. Active hooks (list with triggers)
3. Current configuration highlights
4. Any obvious gaps or opportunities

Format as structured JSON."

Agent 4 - Tech Stack Context:
"From the user's projects and recent work, identify their tech stack:
- Review PROJECTS.md for stated technologies
- Check recent WORK directories for actual usage

Extract and return:
1. Primary languages (TypeScript, Python, etc.)
2. Frameworks in use
3. Deployment targets (Cloudflare, etc.)
4. Key integrations

Format as structured JSON."
```

### Step 2: Launch Thread 2 - Source Collection

Spawn 3 parallel agents to check sources:

```
Use Task tool with subagent_type=Intern, run 3 agents in parallel:

Agent 1 - Anthropic Sources:
"Check Anthropic sources for updates and EXTRACT GRANULAR TECHNIQUES:

Run: bun ~/.claude/skills/PAIUpgrade/Tools/Anthropic.ts

For EACH finding, extract SPECIFIC TECHNIQUES - not summaries:

1. **Release Notes:** For each new feature:
   - Extract the exact syntax/API/configuration
   - Quote the documentation showing how to use it
   - Identify which PAI component this improves

2. **GitHub Commits:** For relevant changes:
   - Extract the actual code pattern or hook signature
   - Show before/after if applicable
   - Map to existing PAI files that could use this

3. **Documentation Updates:**
   - Quote the new content verbatim
   - Identify what capability is now documented that wasn't before
   - Connect to current PAI workarounds this replaces

Return format for EACH technique:
{
  'technique_name': '[Specific name]',
  'source': '[GitHub release/Docs section/Blog post]',
  'exact_content': '[Quoted documentation, code example, or API signature]',
  'current_pai_gap': '[What PAI currently does/lacks that this addresses]',
  'implementation_file': '[Path to PAI file this would change]',
  'code_change': '[Actual code to add/modify]'
}

DO NOT return vague findings like 'new release available'.
EXTRACT the specific techniques from the release.
If something has no concrete technique, skip it with reason."

Agent 2 - YouTube Channels:
"Check configured YouTube channels for new content and EXTRACT GRANULAR TECHNIQUES:

1. Load channel config:
   bun ~/.claude/skills/PAI/Tools/LoadSkillConfig.ts ~/.claude/skills/PAIUpgrade youtube-channels.json

2. For each channel, check recent videos:
   yt-dlp --flat-playlist --dump-json 'https://www.youtube.com/@channelhandle/videos' 2>/dev/null | head -5

3. Compare against state:
   cat ~/.claude/skills/PAIUpgrade/State/youtube-videos.json

4. For NEW videos, extract transcripts:
   bun ~/.claude/skills/PAI/Tools/GetTranscript.ts '<video-url>'

5. CRITICAL - For each transcript, extract SPECIFIC TECHNIQUES:
   - Look for code patterns, configurations, command examples
   - Find timestamps where techniques are explained
   - Quote exact phrases that describe the technique
   - Identify what PAI component this applies to

Return format for EACH technique found:
{
  'technique_name': '[Specific name for the technique]',
  'source_video': '[Video title]',
  'timestamp': '[MM:SS where technique is explained]',
  'exact_quote': '[Direct quote from transcript explaining the technique]',
  'code_or_config': '[Any code/config shown, if applicable]',
  'pai_relevance': '[Which PAI component this could improve]'
}

DO NOT return vague summaries like 'discusses Claude Code features'.
DO NOT recommend watching the video - extract the actual technique.
If a video has no extractable techniques, mark it as 'skipped: no techniques found'."

Agent 3 - Custom Sources:
"Check for any custom sources defined by the user:

1. Look in ~/.claude/skills/PAI/USER/SKILLCUSTOMIZATIONS/PAIUpgrade/
2. Check for additional source definitions beyond YouTube
3. If sources exist, check them for updates

Return: any findings from custom sources.
If no custom sources, return empty list with note 'No custom sources configured'."
```

### Step 2b: Launch Thread 3 - Internal Reflection Mining

Spawn 1 parallel Intern agent alongside Threads 1 and 2:

```
Use Task tool with subagent_type=Intern, run 1 agent in parallel with above:

Agent - Reflection Miner:
"Mine internal algorithm reflections for recurring improvement patterns.

Read MEMORY/LEARNING/REFLECTIONS/algorithm-reflections.jsonl
Parse each line as JSON. For the full MineReflections methodology, see Workflows/MineReflections.md.

Quick summary of what to do:
1. Read all entries from the JSONL file
2. Prioritize entries with implied_sentiment <= 5, within_budget: false, or criteria_failed > 0
3. Cluster Q2 answers (algorithm improvements) into themes by similarity
4. Cluster Q1 answers (execution patterns) into themes
5. For themes with 2+ occurrences (or 1 if sentiment <= 4), create upgrade candidates

Return format:
{
  'entries_analyzed': N,
  'date_range': '[earliest] to [latest]',
  'upgrade_candidates': [
    {
      'theme': '[Theme name]',
      'frequency': N,
      'signal': 'HIGH/MEDIUM/LOW',
      'root_cause': '[Structural issue]',
      'proposed_fix': '[What to change]',
      'target_files': ['[paths]'],
      'supporting_quotes': ['[Q2 excerpts]']
    }
  ],
  'execution_warnings': ['[Recurring Q1 mistakes]'],
  'aspirational_insights': ['[Q3 patterns]']
}

If the reflections file doesn't exist or is empty, return:
{ 'entries_analyzed': 0, 'note': 'No reflections found yet ‚Äî reflections accumulate after Standard+ Algorithm runs' }

EFFORT LEVEL: Return within 60 seconds."
```

### Step 3: Wait and Collect Results

Wait for all 8 agents (4 context + 3 source + 1 reflection) to complete. Collect their outputs.

### Step 4: Synthesize User Context

Merge Thread 1 results into a unified context object:

```json
{
  "user_context": {
    "telos": {
      "current_goals": [...],
      "focus_areas": [...],
      "challenges": [...]
    },
    "recent_work": {
      "active_projects": [...],
      "patterns": [...],
      "open_tasks": [...]
    },
    "pai_state": {
      "skills": [...],
      "hooks": [...],
      "config_highlights": [...]
    },
    "tech_stack": {
      "languages": [...],
      "frameworks": [...],
      "deployment": [...]
    }
  }
}
```

### Step 5: Filter and Score Discoveries

For each discovery from Thread 2:

1. **Relevance check:** Does this relate to user's tech stack? Goals? Projects?
2. **Score relevance:** 1-10 based on match with user context
3. **Score impact:** 1-10 based on capability gained
4. **Score effort:** 1-10 (10 = easy, 1 = hard)
5. **Calculate priority:** (relevance √ó 2) + impact + effort

Filter out discoveries with relevance < 3.

### Step 6: Generate Prioritized Recommendations

Sort by priority score and categorize into FOUR tiers:

- **üî¥ CRITICAL:** Score > 30, relevance > 8. Fixes gaps, security issues, or unlocks capabilities PAI should already have. Integrate immediately.
- **üü† HIGH:** Score 22-30, relevance > 6. Significantly improves PAI capabilities or efficiency. Integrate this week.
- **üü° MEDIUM:** Score 14-21, relevance > 4. Adds useful capabilities or aligns with ecosystem best practices. Integrate when convenient.
- **üü¢ LOW:** Score < 14, or relevance 3-4. Nice-to-know, future reference, or will become relevant later.

For each recommendation, include:
- Short action name (what to do)
- PAI Relevance (WHY it matters for our system ‚Äî this is the primary framing, not an afterthought)
- Effort estimate (Low/Med/High)
- Files affected (specific PAI files that would change)

### Step 7: Output Report

**Discoveries FIRST. Recommendations SECOND. Technique details THIRD.**

Generate the final report following SKILL.md's "Primary Output Format". The report has THREE major sections:

1. **‚ú® Discoveries** ‚Äî Everything interesting found, ranked by coolness/interestingness, with source and PAI relevance. This is the "what's out there" overview {PRINCIPAL.NAME} reads first.
2. **üî• Recommendations** ‚Äî What to actually integrate, organized by four priority tiers.
3. **üéØ Technique Details** ‚Äî Full extracted techniques with code examples as reference.

```markdown
# PAI Upgrade Report
**Generated:** [timestamp]
**Sources Processed:** [N] release notes parsed | [N] videos checked | [N] docs analyzed
**Findings:** [N] techniques extracted | [N] content items skipped

---

## ‚ú® Discoveries

Everything interesting we found, ranked by how compelling it is. This is NOT implementation priority ‚Äî it's "how cool is this."

| # | Discovery | Source | Why It's Interesting | PAI Relevance |
|---|-----------|--------|---------------------|---------------|
| 1 | [Most interesting thing] | [source] | [What makes this cool ‚Äî 1-2 sentences] | [How it maps to PAI] |
| 2 | [Next most interesting] | [source] | [Why it's notable] | [PAI connection] |
| ... | ... | ... | ... | ... |

**Ranking rule:** Sort by genuine interestingness ‚Äî most "whoa" at top. A LOW-priority item can be the #1 most interesting discovery. Interestingness ‚â† implementation priority.

---

## üî• Recommendations

What to actually DO with these discoveries, organized by urgency and impact.

### üî¥ CRITICAL ‚Äî Integrate immediately
[Fixes gaps, security issues, or unlocks capabilities PAI should already have]

| # | Recommendation | PAI Relevance | Effort | Files Affected |
|---|---------------|---------------|--------|----------------|
| [N] | [Short action] | [Why PAI needs this NOW] | [Low/Med/High] | `[files]` |

### üü† HIGH ‚Äî Integrate this week
[Significantly improves PAI capabilities or efficiency]

| # | Recommendation | PAI Relevance | Effort | Files Affected |
|---|---------------|---------------|--------|----------------|

### üü° MEDIUM ‚Äî Integrate when convenient
[Useful capabilities or ecosystem alignment]

| # | Recommendation | PAI Relevance | Effort | Files Affected |
|---|---------------|---------------|--------|----------------|

### üü¢ LOW ‚Äî Awareness / future reference
[Nice-to-know or will become relevant later]

| # | Recommendation | PAI Relevance | Effort | Files Affected |
|---|---------------|---------------|--------|----------------|

---

## üéØ Technique Details

[For EACH technique, numbered to match recommendations above:]

### From [Source Type]

#### [N]. [Technique Name]
**Source:** [Exact source with version/timestamp]
**Priority:** üî¥ CRITICAL | üü† HIGH | üü° MEDIUM | üü¢ LOW

**What It Is (16-32 words):**
[Describe the technique itself. Must be 16-32 words, concrete and specific.]

**How It Helps PAI (16-32 words):**
[Describe the specific PAI benefit. Must be 16-32 words.]

**The Technique:**
> [QUOTE or CODE BLOCK - the actual content, not a summary]

**Applies To:** `[file path]`, [component name]
**Implementation:**
```[language]
// [Before/after or new code]
```

---

## ü™û Internal Reflections

Upgrade candidates mined from our own algorithm reflections (Thread 3). These are recurring patterns in what went wrong or could be improved, based on post-algorithm self-reflection.

**Source:** MEMORY/LEARNING/REFLECTIONS/algorithm-reflections.jsonl
**Entries analyzed:** [N] | **High-signal:** [N] (low sentiment, over-budget, or failed criteria)

[For each upgrade candidate from the reflection miner:]

### [Theme Name] ([N] occurrences, [HIGH/MEDIUM/LOW] signal)
**Root cause:** [What structural issue drives this pattern]
**Proposed fix:** [Concrete change]
**Target:** [PAI files affected]
**Evidence:**
- [timestamp] [task] ‚Äî "[Q2 quote]"

[If no reflections exist yet:]
> No reflections found yet ‚Äî they accumulate after Standard+ Algorithm runs. Run the Algorithm a few more times and this section will populate.

---

## üìä Summary

| # | Technique | Source | Priority | PAI Component | Effort |
|---|-----------|--------|----------|---------------|--------|
[Table with priority emoji column ‚Äî include internal reflection candidates]

**Totals:** [N] Critical | [N] High | [N] Medium | [N] Low | [N] Skipped | [N] Internal

---

## ‚è≠Ô∏è Skipped Content

| Content | Source | Why Skipped |
|---------|--------|-------------|

---

## üîç Sources Processed

[What was actually analyzed, with extraction counts]
```

**CRITICAL Output Rules:**
1. **DISCOVERIES TABLE COMES FIRST** - The ‚ú® Discoveries table is the very first section after the header. This is the comprehensive "what's out there" overview ranked by interestingness. It should feel like a substantial inventory of everything cool that was found.
2. **DISCOVERIES ‚â† RECOMMENDATIONS** - The Discoveries table ranks by "how cool is this" (interestingness). The Recommendations section ranks by "how urgently should PAI integrate this" (priority). These are DIFFERENT orderings. A LOW-priority item can be Discovery #1 if it's the most interesting thing found.
3. **RECOMMENDATIONS COME SECOND** - The üî• Recommendations section with four priority tiers follows the Discoveries table.
4. **PAI RELEVANCE IS PRIMARY** - In BOTH the Discoveries table AND the Recommendation tables, explain WHY this matters for PAI.
5. **FOUR TIERS, NO EXCEPTIONS** - Every technique maps to exactly one of: üî¥ CRITICAL, üü† HIGH, üü° MEDIUM, üü¢ LOW. Empty tiers are fine ‚Äî show them anyway.
6. **NUMBERED CROSS-REFERENCES** - Techniques are numbered consistently across Discoveries, Recommendations, and Technique Details.
7. **QUOTE THE SOURCE** - Every technique must include actual quoted content or code
8. **MAP TO PAI** - Every technique must name a specific PAI file or component it improves
9. **TWO MANDATORY DESCRIPTIONS (16-32 words each):**
   - **What It Is:** Describe the technique itself - what it does, how it works, what capability it provides
   - **How It Helps PAI:** Describe the specific benefit - which component improves, what gap it fills
10. **NO WATCH/READ RECOMMENDATIONS** - Extract the technique, don't point to the content
11. **SKIP BOLDLY** - If content has no technique, skip it; don't dilute with summaries
12. **WORD COUNT ENFORCEMENT** - Count words in description fields. Under 16 = add specificity. Over 32 = condense.

### Step 8: Update State

Update state files to avoid duplicate processing:
- `State/last-check.json` - Updated by Anthropic.ts tool
- `State/youtube-videos.json` - Add newly processed video IDs

---

## Quick Mode

If user says "check Anthropic only" or similar:
- Skip Thread 1 (use cached context if available from session)
- Run only the relevant Thread 2 agent
- Apply lighter filtering
- Output abbreviated report

---

## Error Handling

- If Thread 1 agents fail: Proceed with minimal context, note in output
- If Thread 2 agents fail: Report which sources couldn't be checked
- If no discoveries: Output "No new updates found" with sources checked
- If all discoveries filtered: Output "Updates found but none relevant to current focus"

---

## Integration

**Input workflows:**
- Can be triggered automatically on schedule (cron)
- Can be triggered by user command

**Output workflows:**
- Discoveries feed into **ResearchUpgrade** for deep dives
- Recommendations can generate todos
- Can trigger implementation workflows

---

## Example Execution

```
User: "check for upgrades"

[Agents run in parallel...]

# PAI Upgrade Report
**Generated:** 2026-01-15 19:45:00 PST
**Sources Processed:** 20 release notes parsed | 5 videos checked | 30 docs analyzed
**Findings:** 3 techniques extracted | 4 content items skipped

---

## ‚ú® Discoveries

Everything interesting we found, ranked by how cool it is.

| # | Discovery | Source | Why It's Interesting | PAI Relevance |
|---|-----------|--------|---------------------|---------------|
| 1 | PreToolUse hooks can inject reasoning context | claude-code v2.1.16 | Hooks can now return `additionalContext` that Claude reasons about before tool execution ‚Äî this is a paradigm shift from binary block/allow to intelligent security | SecurityValidator could inject warnings instead of blocking, enabling context-aware security decisions |
| 2 | Native ${CLAUDE_SESSION_ID} variable | claude-code v2.1.16 | Session IDs are now first-class environment variables everywhere ‚Äî no more extraction hacks | Session documentation workflows can drop manual ID extraction code |
| 3 | MCP auto mode enabled by default | claude-code v2.1.16 | MCP servers now auto-connect without explicit configuration | Already enabled ‚Äî no action needed |

---

## üî• Recommendations

### üî¥ CRITICAL ‚Äî Integrate immediately

| # | Recommendation | PAI Relevance | Effort | Files Affected |
|---|---------------|---------------|--------|----------------|
| 1 | Add PreToolUse additionalContext to security hooks | SecurityValidator currently hard-blocks commands ‚Äî additionalContext enables reasoning-based security that adapts to context | Low | `hooks/SecurityValidator.hook.ts` |

### üü† HIGH ‚Äî Integrate this week

| # | Recommendation | PAI Relevance | Effort | Files Affected |
|---|---------------|---------------|--------|----------------|
| 2 | Replace session ID hacks with native ${CLAUDE_SESSION_ID} | Session documentation workflows have manual extraction workarounds ‚Äî native variable eliminates fragile code | Low | `skills/_SYSTEM/Workflows/DocumentSession.md` |

### üü° MEDIUM ‚Äî Integrate when convenient

(none this run)

### üü¢ LOW ‚Äî Awareness / future reference

(none this run)

---

## üéØ Technique Details

### From Release Notes

#### 1. PreToolUse Additional Context
**Source:** GitHub claude-code v2.1.16
**Priority:** üî¥ CRITICAL

**What It Is (16-32 words):**
PreToolUse hooks can now return an additionalContext field that gets injected into the model's context before tool execution, enabling reasoning-based security rather than hard blocks.

**How It Helps PAI (16-32 words):**
SecurityValidator.hook.ts currently blocks dangerous commands. With additionalContext, it can inject warnings Claude reasons about, enabling smarter security that adapts to context.

**The Technique:**
```typescript
return { decision: "allow", additionalContext: "WARNING: Protected file." };
```

**Applies To:** `hooks/SecurityValidator.hook.ts`

---

#### 2. Session ID Substitution
**Source:** GitHub claude-code v2.1.16
**Priority:** üü† HIGH

**What It Is (16-32 words):**
Native environment variable ${CLAUDE_SESSION_ID} is now available in all hooks and commands, eliminating the need for custom session ID extraction or workaround code.

**How It Helps PAI (16-32 words):**
Our session documentation workflows had manual session ID extraction hacks. Native substitution means cleaner code and reliable session tracking across all PAI workflows.

**The Technique:**
```bash
echo "Session: ${CLAUDE_SESSION_ID}"
```

**Applies To:** `skills/_SYSTEM/Workflows/DocumentSession.md`

---

## üìä Summary

| # | Technique | Source | Priority | PAI Component | Effort |
|---|-----------|--------|----------|---------------|--------|
| 1 | PreToolUse Additional Context | claude-code v2.1.16 | üî¥ | SecurityValidator hook | Low |
| 2 | Session ID Substitution | claude-code v2.1.16 | üü† | DocumentSession workflow | Low |

**Totals:** 1 Critical | 1 High | 0 Medium | 0 Low | 4 Skipped

## ‚è≠Ô∏è Skipped Content

| Content | Source | Why Skipped |
|---------|--------|-------------|
| MCP auto mode | claude-code v2.1.16 | Already enabled by default |
| Gemini 3 videos | YouTube | Not relevant to Claude-centric stack |
| Agent Experts video | YouTube | No concrete technique identified |
| SDK update v0.78 | GitHub | PAI uses CLI, not raw SDK |

## üîç Sources Processed
30 Anthropic sources, 5 YouTube videos, 0 custom ‚Üí 2 relevant findings
```

---

**This workflow implements the core PAIUpgrade value proposition: understanding YOU first, discovering what's new second, then connecting them into actionable, personalized upgrades.**
